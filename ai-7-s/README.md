# Neural Network  

## Contents
- ニューラルネットワークの基礎概念
- ニューラルネットワークを使った教師なし学習
- ニューラルネットワークを使った教師あり学習
- 深層学習の概要


## ニューラルネットワークの基礎概念

**ニューラルネットワーク**は生物の神経系をヒントにして考案された学習モデルである。つまり、**ニューロン**の構造とシナプスにおける電気パルスの伝達を計算モデルで表現している。**パーセプトロン**と名付けられた単純なユニットが多数結合され、それぞれの結びつきに**結合荷重**が定義され、この結合の強さを学習の過程で随時更新することで、学習器としての機能を高めていく。  

キーワード：荷重和、活性化関数、閾値、ニューロン（ノード）、シナプス（結合荷重のこと）  
ちなみに、荷重和は次の式で表される： ((∑ i=0,n) (w(i)\*x(i) - 閾値))  

#### 活性化関数について  
- ステップ関数  
閾素子モデルともいう。入力が正なら１を出力し、０か負であれば０を出力するような関数。直感的でありまた生物のニューロンに仕組みが近いことから、研究の初期の頃はよく使われていた。が、連続でなければ０で微分可能でもないため、色々と厄介である。  
- 準線形素子モデル  
またの名をシグモイド関数という。シグモイド関数であれば、どこでも微分可能だし連続。そのため、計算が簡単になる。シグモイド関数は、次式で表される： ( 1 / ( 1 + exp(-y) ) )  
- 原点を通るシグモイド関数  
一つ上で紹介したシグモイド関数は２次元のグラフに描画した時に原点を通らない。これは何かと不都合である。そのため、原点を通るバージョンが考案された。これは次式で表される： f(y) = tanh(y)  
シグモイド関数を２倍にして、y軸方向に−１だけ移動した関数と考えることもできる。  
- ReLU  
正式名用は、Rectified Linear Unit。ランプ関数とも呼ばれる。０付近で微分可能ではないものの、そこでの変化は緩慢であり、シグモイド関数の難点を補っていると言える（らしい）。  
式は次の形で書ける： f(y) = max(0,y)  
- ソフトプラス関数  
英語での名称はsoftplus（そのまんま）。これは、ReLU関数を近似し、連続微分可能としたものである。  
式は次のように書ける：  f(y) = log(1 + e^y)  

#### ニューロンとシナプスの演算例
ニューロンとシナプスの演算例を**neuron.c**に示した。

#### シナプスの結合荷重の学習
