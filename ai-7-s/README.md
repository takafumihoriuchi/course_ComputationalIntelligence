# Neural Network  

## Contents
- ニューラルネットワークの基礎概念
- ニューラルネットワークを使った教師なし学習
- ニューラルネットワークを使った教師あり学習
- 深層学習の概要


## ニューラルネットワークの基礎概念

**ニューラルネットワーク**は生物の神経系をヒントにして考案された学習モデルである。つまり、**ニューロン**の構造とシナプスにおける電気パルスの伝達を計算モデルで表現している。**パーセプトロン**と名付けられた単純なユニットが多数結合され、それぞれの結びつきに**結合荷重**が定義され、この結合の強さを学習の過程で随時更新することで、学習器としての機能を高めていく。  

キーワード：荷重和、活性化関数、閾値、ニューロン（ノード）、シナプス（結合荷重のこと）  
ちなみに、荷重和は次の式で表される： ((∑ i=0,n) (w(i)\*x(i) - 閾値))  

#### 活性化関数について  
- ステップ関数  
閾素子モデルともいう。入力が正なら１を出力し、０か負であれば０を出力するような関数。直感的でありまた生物のニューロンに仕組みが近いことから、研究の初期の頃はよく使われていた。が、連続でなければ０で微分可能でもないため、色々と厄介である。  
- 準線形素子モデル  
またの名をシグモイド関数という。シグモイド関数であれば、どこでも微分可能だし連続。そのため、計算が簡単になる。シグモイド関数は、次式で表される： ( 1 / ( 1 + exp(-y) ) )  
- 原点を通るシグモイド関数  
一つ上で紹介したシグモイド関数は２次元のグラフに描画した時に原点を通らない。これは何かと不都合である。そのため、原点を通るバージョンが考案された。これは次式で表される： f(y) = tanh(y)  
シグモイド関数を２倍にして、y軸方向に−１だけ移動した関数と考えることもできる。  
- ReLU  
正式名用は、Rectified Linear Unit。ランプ関数とも呼ばれる。０付近で微分可能ではないものの、そこでの変化は緩慢であり、シグモイド関数の難点を補っていると言える（らしい）。  
式は次の形で書ける： f(y) = max(0,y)  
- ソフトプラス関数  
英語での名称はsoftplus（そのまんま）。これは、ReLU関数を近似し、連続微分可能としたものである。  
式は次のように書ける：  f(y) = log(1 + e^y)  

#### ニューロンとシナプスの演算例
ニューロンとシナプスの演算例を**neuron.c**に示した。main関数の中のprintf関数で、add、or、notを表現したニューロンを稼働させている。

#### シナプスの結合荷重の学習
ニューロンが複数結合することにより、**ニューラルネットワーク**を構成する。このニューラルネットワークの**学習**とは、シナプスの結合荷重を、次の計算式で随時更新することをいう。  

> wi(t+1) = k\*wi(t) + ∆wi(t) (0 < k < 1)  
> ∆wi(t) = α\*δ\*xi  

なお、wi(t)とは、t回目の学習のときの荷重であり、教師データセットが一つ与えられるたびに、新たにwi(t+1)を計算し、重みwiを更新する。xiはwiに対応した入力。αとkは、学習効率を決定する係数であり、またδは学習信号と呼ばれる値。  

ニューラルネットワークの種類には、階層型ニューラルネットワーク、再帰型ニューラルネットワークなどがある。  


## ニューラルネットワークを使った教師なし学習

近年の主流は、教師あり学習だが、ここでは軽く教師なし学習のニューラルネットワークについて解説する。  
教師なし学習の一つに「Hebbの学習」がある。これでは、先にδと出力xについて、  

> δ＝x  

と定義する。つまり、あるニューロンの出力が大きくなると、その入力も大きくなるように荷重が増加されるということ。この学習では、類似したある特定のパターンの出力が学習され、反応しやすくなる効果がある。これは結果として、クラスタリングのような分類・抽出をすることを意味する。自己組織化写像のようなアルゴリズムは、この学習手法を用いて二次元平面状に分類を行う。どうアルゴリズムの詳細は割愛する。


## ニューラルネットワークを使った教師あり学習

こちらが現在の主流となっている学習方法。ニューラルネットワークの教師データによる学習は1957年にF.Rosenbrattによって提案された。この学習方法では、学習器に入力データとともに「教師データ（教師信号、訓練データとも）」と呼ばれる正解ラベルを与え、層と層の間の荷重を調整することで学習を行う。この学習の過程を**誤り訂正学習**という。  
  
ここではまず初めに、**入力層＋中間層＋出力層**という三層構造の**パーセプトロン**を利用して解説を行う。  
ここで鍵となるのは**δ**（δ = d-x）の存在である。ただし、xは推測値として得られた出力であり、dは教師データとして与えられた正解ラベルである。この計算によって得られたδを前出の式  

> wi(t+1) = k\*wi(t) + ∆wi(t) (0 < k < 1)  
> ∆wi(t) = α\*δ\*xi  

に適用することで、荷重wを更新することが基本的な流れである。パーセプトロンの複数の層の荷重の更新に取り掛かる前に、手馴しを兼ねて２入力のニューロンの誤り訂正学習を考えよう。C言語のプログラム**neuron_update_weight.c**を参照。（neuron_update_weight_(1~3).cは各パラメータの値を変更して実行したもの）  

#### 誤差逆伝播法
ここからは、教師ありニューラルネットワークに関して本講義ノートで扱う項目の中で最重要項目である「誤差逆伝播法(back propagation)」について説明する。数行上で触れたパーセプトロンに対する誤り訂正学習は出力層とその手前の層の間の荷重を修正するのみであるため、学習能力が低い。複雑な計算を行うためには、層の数を増やす必要があり、荷重の更新を手前の層にまで適用する必要がある。  
  
入力層、複数の中間層、出力層からなるニューラルネットワークに、入力とそれに対する教師データが与えられるとする。また、ここでは活性化関数にシグモイド関数（ f(y)=(1/1+exp(-y)) ）を用いる。ステップ関数とは異なり、シグモイド関数は微分可能である。  
  
**手順１：**
角層間の荷重を０に近い数字に初期化する。（初期化の手法は様々なものが研究されているが、ここでは便宜上、ランダムに初期化を行うことにする。）  
  
**手順２：**
入力データを入力層に入力し、各層でニューロンの出力計算をする。これらは前出の計算方法と同じ。次の手順からが誤差逆伝播法の本質。  
  
**手順３：**
出力層に注目する。出力と教師データとの二乗和誤差E（損失誤差ともいう）を計算する。Eの計算式は次の通り。
> E = (1/2) \* Σ(k=1~L)(dk - zk)^2  
  

**手順４：**
Eを最小にするべく（つまり出力を教師データに一致させたい）、出力層への入力にかかる結合荷重を調整する。この荷重をvjkで表すと、計算式は以下の通り。
> vjk(t+1) = k \* vjk(t) + ∆vjk  
> ∆vjk = α \* **δk** \* xj  

∆vjkを上のように書いたが、これは元々は次のような計算により導出される。
> ∆vjk = -α \* (∂E/∂vjk)  
> .... = -α \* (∂E/∂zk)(∂zk/∂yk)(∂yk/∂vjk)  
> .... = α \* **(dk-zk) \* f'(yk)** \* xj  

つまり、二乗和誤差を元々の重みで偏微分したものである。そして、上の式でボールドになっているところを**δk**と定義する。すると、δkは次のように変形できる。
> δk = (dk-zk) \* f'(yk)  
> .. = (dk-zk) \* f(yk) * (1 - f(yk))  

αは別途調整する変更料を表すパラメータであるから、「**∆vjk=α\*δk\*xj**」 の数値を計算できるようになる。これにより、「vjk(t+1)=k\*vjk(t)+∆vjk」が計算でき、荷重vjkが更新される。  
  
**手順５：**
今度は、出力層から一つ手前の層と、そのさらに一つ手前の層との間の結合荷重を調整することを考える。つまり、中間層の学習。ここでも、出力層直前の結合荷重の調整と同様に、二乗和誤差Eを小さくするような計算でここでの結合荷重wijの調整を行うが、Eの計算に使用した出力は出力層のものなので、これを考慮する必要がある。つまり、Eを合成関数として扱う。結合荷重の更新は次のような計算式で得られる。
> ∆wjk = -α \* (∂E/∂wjk)  
> .... = -α \* Σ(k=1~L) \{ (∂E/∂zk)(∂zk/∂yk)(∂yk/∂xj)(∂xj/∂qj)(∂qj/∂vjk) \}  
> .... = α \* Σ(k=1~L) \{ (dk-zk)\*f'(yk) \*vjk\*f'(qj)\*pi \}  
> .... = α \* Σ(k=1~L) \{ δk\*vjk\*f'(qj)\*pi \}  

**手順６：**
ここで手順４のδkの計算と同じ要領で**δ'j**を計算する。
> δ'j = Σ(k=1~L) \{ δk\*vjk\*f'(qj) \}  
> ... = Σ(k=1~L) \{ δk\*vjk\*f(qj)\*(1-f(qj)) \}  

これで、上記の∆wjkを計算することができるようになり、結合荷重wjkが更新される。  
  
**手順７：**
手順５〜６を入力層に達するまで繰り返す。  
  
**手順８：**
手順７までを各入力と教師データのついに対して繰り返す。  
  
以上が誤差逆伝播法の手順。出力層から入力層まで全ての結合荷重が更新されるため、学習効率が高い。この手法があるからこそ、ディープな学習が可能となる。  

