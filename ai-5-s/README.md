# Reinforcement Learning  


## Contents
- 動的計画法の復習
- 確率の復習
- 基本事項
- 導入
- TD学習法（時間的差分学習法）
- Q学習（行動価値の学習）  
  
このセクションの核心は強化学習の基本的アルゴリズムである「TD法」と「Q学習」を理解すること。


## 動的計画法

割愛。


## 確率の復習

割愛。


## 基本事項

まず、強化学習とは何か。  
何らかの良し悪しを認知して、選択した行為を評価する学習方法。より明確には、「エージェントが自ら行動し、その結果に応じて**報酬**を得て、最適な行為を獲得する学習」である。
ここで、エージェントにあらかじめ知識が与えられることはない。つまり、教師あり学習のようにあらかじめ正負の例が与えられることはない。（ただし、場合によっては地図に相当するモデルが与えられることはある。）エージェントは事前情報がないまま行動し、行動の直後もしくは全ての行動が終わった後に得られる報酬を受け取ることで学習を行う。学習の対象は、「環境の状態の**価値**」または「各状態でどの行為が**価値**があるか」である。「価値」とは、将来を含めて得られるであろう報酬の累積のこと。エージェントはこの価値を高めるために行動を試みる。  
  
以下、強化学習の詳細を見ていくにあたり必要となる定義などの確認を行う。

#### マルコフ決定過程  

環境の**状態**の集合  
**S = {s1, s2, ..., sn}**  
エージェントの**行為**の集合  
**A = {a1, a2, ..., an}**  
時間tにおいて、環境s(t)にあるエージェントが行為a(t)をとると、環境は確率的に状態s(t+1)に遷移する。条件付き確率を次の**状態遷移確率**で表す。  
P( s(t+1) | s(t), a(t) )  
さらにこれを次の形で表す。  
**Pr( s(t), a(t), s(t+1) )**  
また、この状態遷移と同時に得られる**報酬の期待値**を次の形で表す。  
**r( s(t), a(t) )**  
ここで、Pr()とr()がその時のsとaにのみ依存し、それ以前の状態や行為とは無関係である性質を**マルコフ性**という。また、以上のように環境のダイナミズムをモデル化したものを**マルコフ決定過程**（Markov Decision Process, MDP）という。  

状態遷移確率Pr()は  
Pr : SxAxS -> [0,1]  
なる関数とも考えられる。ここで、全ての状態遷移確率の和は１となる。また、ある状態において、全ての行為に対する状態遷移確率のうち、どれか一つのみが１となり、他は全て０であるとき、マルコフ決定過程は決定的であるといい、状態遷移が一意に決まることを意味する。  

#### 強化学習の基本設計

強化学習では、エージェントが「状態」または「行為」の価値を学習するように設計する。  
**状態sの価値**を表す関数を**価値関数**といい、**V(s)**で表す。  

また、ある状態における**行為の価値**を学習することを**Q学習**という。  
**状態sにおける行為aの価値関数**を**Q(s,a)**で表す。  
  
環境の**状態から行為を選択する**関数を**政策関数**という。（policy、方策関数ともいう。）  
π : S -> A  
強化学習とは、高い累積報酬を目指した政策関数を学習することである。  

## 導入

#### 行動選択の戦略とジレンマ
「学習の効率」と「獲得報酬」のトレードオフ。それまでに学習したことを活用して、その範囲内での最善策を選択するか、もしくはさらなる大きい報酬を得る可能性を探索するために他の選択肢を試すか、というジレンマ。

#### 行動選択の戦略
- **Greedy戦略**とは、今までに蓄えた知識に執着し、新しい試みは決してしないという戦略。知っている範囲内で最善のものを選択するため、博打を打って失敗する、というようなことはない。具体的には、すでに得られた価値関数Q(s,a)が最大となるように行動aを実行する、もしくは価値関数V(s)が最大となるような状態sに移動する。
- **ε-greedy戦略**とは、大抵の場合はgreedy戦略を取りながら、ある確率εで、ランダムな行動aをとるような戦略。通常、εは０に近い値を設定する。  



## TD学習法（時間的差分学習法）

TD学習法とは"Temporal Difference learning method"の略式名称。時間的差分学習法はUpdateのアルゴリズムとして、学習の収束も比較的速く、計算量も小さいため、最小二乗法や適応動的計画法よりも広く使われている。「Updateのアルゴリズム」とは、既知環境における受動的学習で価値関数Vを更新する計算のこと。  

ここで、「**ある状態sの価値V(s)とは**なんなのか」を考えると、それは**その状態以降にある政策 に基づいて行動を進めると得られる報酬の期待累積値**であると言える。すなわち、ある状態sから状態s'に移動するときに得られる報酬をrとすると、V(s)は未来も含めた累積であるため、「V(s)」は「rとV(s')の和」であると考えられる。V(s)を正しい値へと更新するときは、V(s)の値を「rとV(s')の和」に少しだけ近づける。「近づける」なのは、今後の修正で「rとV(s')の和」が変更される可能性があるから。この時に近づける割合を学習率αで表す。終了状態sgの価値は観測可能であるため、V(sg)が最初に分かり、逆方向に随時更新していく流れとなる。  

**割引報酬**とは、今の状態から何ステップ先の行為かにより、報酬を調整すること。つまり、今の状態から近い行為はあまり割引されず、遠くの行為は、たとえそれが成功に近い状態であったとしても、割引される。これにより、より短いステップでの成功の方が、長いステップでの成功よりも評価される。数式で表すと次のようになる。ただし、γ(0<=γ<=1)である。
> V( s(t) ) = ∑(n=0からgoal) { γ^n \* r( s(t+n), a(t+n) ) }  


## Q学習（行動価値の学習）

Q学習とは、**Q値**を学習すること。Q値とは、状態siでの行為aの価値で、Q(si,a)で表す。つまり、ある状態で、どの行為が大きな累積報酬期待値をもたらすかを直接学習する。  
Q学習で学習するものは、政策関数πであり、これは状態の集合Sから行為の集合Aへの写像である。つまり、状態と行為を結ぶルールであると考えられる。これがあるため、状態の遷移を表すモデルMを用意する必要がない。これは、Q値に状態遷移の情報も含まれるからである。  
  
以下省略