# Reinforcement Learning  


## Contents
- 動的計画法の復習
- 確率の復習
- 基本事項
- 導入
- TD法（時間的差分学習法）
- Q学習（行動価値の学習）  
  
このセクションの核心は強化学習の基本的アルゴリズムである「TD法」と「Q学習」を理解すること。


## 動的計画法




## 確率の復習




## 基本事項

まず、強化学習とは何か。  
何らかの良し悪しを認知して、選択した行為を評価する学習方法。より明確には、「エージェントが自ら行動し、その結果に応じて**報酬**を得て、最適な行為を獲得する学習」である。
ここで、エージェントにあらかじめ知識が与えられることはない。つまり、教師あり学習のようにあらかじめ正負の例が与えられることはない。（ただし、場合によっては地図に相当するモデルが与えられることはある。）エージェントは事前情報がないまま行動し、行動の直後もしくは全ての行動が終わった後に得られる報酬を受け取ることで学習を行う。学習の対象は、「環境の状態の**価値**」または「各状態でどの行為が**価値**があるか」である。「価値」とは、将来を含めて得られるであろう報酬の累積のこと。エージェントはこの価値を高めるために行動を試みる。  
  
以下、強化学習の詳細を見ていくにあたり必要となる定義などの確認を行う。

#### マルコフ決定過程  

環境の**状態**の集合  
**S = {s1, s2, ..., sn}**  
エージェントの**行為**の集合  
**A = {a1, a2, ..., an}**  
時間tにおいて、環境s(t)にあるエージェントが行為a(t)をとると、環境は確率的に状態s(t+1)に遷移する。条件付き確率を次の**状態遷移確率**で表す。  
P( s(t+1) | s(t), a(t) )  
さらにこれを次の形で表す。  
**Pr( s(t), a(t), s(t+1) )**  
また、この状態遷移と同時に得られる**報酬の期待値**を次の形で表す。  
**r( s(t), a(t) )**  
ここで、Pr()とr()がその時のsとaにのみ依存し、それ以前の状態や行為とは無関係である性質を**マルコフ性**という。また、以上のように環境のダイナミズムをモデル化したものを**マルコフ決定過程**（Markov Decision Process, MDP）という。  

状態遷移確率Pr()は  
Pr : SxAxS -> [0,1]  
なる関数とも考えられる。ここで、全ての状態遷移確率の和は１となる。また、ある状態において、全ての行為に対する状態遷移確率のうち、どれか一つのみが１となり、他は全て０であるとき、マルコフ決定過程は決定的であるといい、状態遷移が一意に決まることを意味する。  

#### 強化学習の基本設計

強化学習では、エージェントが「状態」または「行為」の価値を学習するように設計する。  
**状態sの価値**を表す関数を**価値関数**といい、**V(s)**で表す。  

また、ある状態における**行為の価値**を学習することを**Q学習**という。  
**状態sにおける行為aの価値関数**を**Q(s,a)**で表す。  
  
環境の**状態から行為を選択する**関数を**政策関数**という。（policy、方策関数ともいう。）  
π : S -> A  
強化学習とは、高い累積報酬を目指した政策関数を学習することである。  

## 導入

#### 行動選択の戦略とジレンマ

「学習の効率」と「獲得報酬」のトレードオフ（＊）。それまでに学習したことを活用して、その範囲内での最善策を選択するか、もしくはさらなる大きい報酬を得る可能性を探索するために他の選択肢を試すか、というジレンマ。~（＊「トレードオフ」という言葉を使うとよくわかっている風になる）~


## TD法（時間的差分学習法）




## Q学習（行動価値の学習）



